Walkthrough for users and maintainers
==========================================

> {-# LANGUAGE DeriveAnyClass #-}
> {-# LANGUAGE DeriveGeneric #-}
> {-# LANGUAGE DerivingStrategies #-}
> {-# LANGUAGE OverloadedLabels  #-}
> {-# LANGUAGE OverloadedLists   #-}
> {-# LANGUAGE OverloadedStrings #-}
> {-# LANGUAGE StandaloneDeriving #-}

> module Walkthrough where
> import Data.Aeson (Value)
> import Data.Aeson.Encode.Pretty (encodePretty)
> import Data.ByteString.Lazy.Char8 (putStrLn)
> import Data.Either
> import Data.Maybe (fromMaybe)
> import Data.Generics.Labels
> import qualified Generics.SOP as SOP
> import GHC.Generics (Generic)
> import Schemas
> import Schemas.Internal
> import Schemas.SOP
> import Text.Pretty.Simple

Motivation
----------

Consider a service exposing a JSON endpoint with the Response type below,
which includes additional debug data not intended for sharing with the callers of this service.

How would one use *schemas* to encode this response while leaving out the debug data?

> data Mode = ModeI Int | ModeB Bool   deriving Show

> data Response = Response {
>   responseId :: Int,
>   mode :: Mode,
>   responseDetails :: Maybe String,
>   responseDebugData :: String
> } deriving Show

> exampleResponse :: Response
> exampleResponse = Response 1 (ModeB True) (Just "details") "debug"

The first thing to do is defining a typed schema, which can be automated using generics:

> deriving instance Generic Mode
> deriving anyclass instance SOP.Generic Mode
> deriving anyclass instance SOP.HasDatatypeInfo Mode
> instance HasSchema Mode where schema = gSchema defOptions

> deriving instance Generic Response
> deriving anyclass instance SOP.Generic Response
> deriving anyclass instance SOP.HasDatatypeInfo Response
> instance HasSchema Response where schema = gSchema defOptions

Defining the schemas manually is also possible if more control is desired.

> modeSchema :: TypedSchema Mode
> modeSchema = union
>     [ ("I", alt #_ModeI)   -- The funny hash indicates a label which
>     , ("B", alt #_ModeB)   -- desugars to a prism via Data.Generics.Labels
>     ]

> responseSchema :: TypedSchema Response
> responseSchema = record $ Response
>   <$> field "id" responseId
>   <*> fieldWith modeSchema "mode" mode
>   <*> optField "details" responseDetails
>   <*> field "debugInfo" responseDebugData

'TypedSchema' is more than a schema, it's a formula that can be used to derive
encoders, decoders, as well as a concrete schema (or many if the typed schema
makes use of alternatives).

The next step is to define the concrete schema that the client will use when
making a request to the server. The client schema defines the type of the
response that the client expects to receive from us. It must be a subtype of the
schema generated by the `extractSchema` interpretation.

```
*Main Text.Pretty.Simple NE> s = NE.head $ extractSchema responseSchema
*Main Text.Pretty.Simple NE> :t s
s :: Schema
*Main Text.Pretty.Simple NE> pPrint s
{ debugInfo :: String
, mode :: { B :: Boolean } | { I :: Integer }
, details ?? String
, id :: Integer
}
```

In our case, before using this schema we want to remove the `debugInfo` field,
so that it will not be included in the response. We can make this edit directly
on the JSON representation of the untyped schema above, which can be produced
by using `Schemas.encode`:

```
*Main Text.Pretty.Simple NE B> B.putStrLn $ Data.Aeson.Encode.Pretty.encodePretty $ encode s
{
    "Record": {
        "debugInfo": {
            "schema": {
                "Prim": "String"
            }
        },
        "mode": {
            "schema": {
                "Union": [
                    {
                        "schema": {
                            "Prim": "Boolean"
                        },
                        "constructor": "B"
                    },
                    {
                        "schema": {
                            "Prim": "Integer"
                        },
                        "constructor": "I"
                    }
                ]
            }
        },
        "details": {
            "schema": {
                "Prim": "String"
            },
            "isRequired": false
        },
        "id": {
            "schema": {
                "Prim": "Integer"
            }
        }
    }
}
```

After deleting the `debugInfo` field, the final schema will be:

```
{
    "Record": {
        "mode": {
            "schema": {
                "Union": [
                    {
                        "schema": {
                            "Prim": "Boolean"
                        },
                        "constructor": "B"
                    },
                    {
                        "schema": {
                            "Prim": "Integer"
                        },
                        "constructor": "I"
                    }
                ]
            }
        },
        "details": {
            "schema": {
                "Prim": "String"
            },
            "isRequired": false
        },
        "id": {
            "schema": {
                "Prim": "Integer"
            }
        }
    }
}
```

We can verify that the modified schema is a valid target using the predicate:

> isValidTargetResponseSchema :: Schema -> Bool
> isValidTargetResponseSchema = isLeft . encodeToWith responseSchema

To implement the endpoint, we need to obtain and decode the requested schema,
then use it to drive the encoding of a `Response` value, as shown by the code
below:

> endpoint :: Value -> Response -> Either String Value
> endpoint targetSchema response =
>     case encodeToWith responseSchema decodedTargetSchema of
>       Left mismatch ->
>         -- the source and target schemas are incompatible
>         Left (show mismatch)
>       Right encoder ->
>         Right (encoder response)
>   where
>     -- run with up to 1000 recursive steps
>     decodedTargetSchema =
>        fromMaybe (error "ran out of gas") $
>          either (error.show) id $
>            runResult 1000 $ decode targetSchema

To decode the produced JSON in the client-side end, one can use *schemas* perhaps, or
 a hand-rolled JSON parser. Currently schemas offers some support for deriving
 OpenApi specifications in the `Schemas.OpenApi` module.

TypedSchema interpretations
-------------------------------

A `TypedSchemaFlex from to` is a formula which can be given the following interpretations:

- `encodeToWith`  (target :: Schema) :: from -> Aeson.Value        -- an encoder for a target schema
- `decodeFromWith`(source :: Schema) :: Aeson.Value -> Maybe to    -- a decoder from a source schema
- `extractSchema`                    :: [Schema]                   -- a set of concrete schemas
- `runSchema`                        :: from -> Either Mismatch to -- a projection

The `runSchema` projection is total iff the `TypedSchemaFlex from to ` formula is total,
and in the case where `from ~ to` it should be the identity function.

> id_response :: Response -> Response
> id_response = either (error . show) id . runSchema responseSchema

Backwards compatible changes
--------------------------------

By decoupling the source and target schemas, we can make modifications to either
of them in a backwards compatible way. For instance the following modifications
are backwards compatible for encoding:

- remove fields from target schema
- remove optional fields from source schema
- add fields to source schema
- add optional fields to response schema
- add alternatives to enum/union in response schema
- remove alternatives from enum/union in source schema

In this context, changes to the source schema really mean changes to the Haskell
datatype and to the corresponding `TypedSchema` value, and changes to the target
schema mean edits to the `Schema` value.

For example, below is the sequence of steps to add a new mandatory field:

1. Extend the Haskell type and the corresponding `TypedSchema` (the source schema)
2. Prove backwards compatibility via the predicate `isRight (encodeTo targetSchema)`
3. Redeploy the service
4. Eventually, update the clients with new target schemas that include the new field.

The key point is that 3 and 4 need not happen at the same time, as the
updated service is still able to serve clients using the old target schema.

The addition of the `religion` field in the `Person2` example is a backwards
compatible change.

Non backwards compatible changes
--------------------------------

Examples of non backwards compatible changes for encoding:

- renaming of fields / alternatives
- removal of mandatory fields from source schema
- addition of mandatory fields in target schema

The `Monoid TypedSchema` instance can be used to provide multiple schemas for a
type, which can be used to implement non backwards compatible changes. Similarly,
applicative record schemas have an `Alternative` instance for this.
The search is greedy, so ordering of alternatives matter.

The renaming of the `education` field in the `Person2` example is a non backwards
compatible change.

Recursive datatypes
-------------------

Schemas for recursive datatypes need to make use of the `named` combinator
as seen in the [Person3](examples/Person3.hs) example. This is currently
not automatically inserted by `gSchema`.

`Person3` contains an example of a recursive datatype.

Recursive data
--------------

While recursive datatypes are supported, recursive data is not. For instance,
the default schema for `Schema` is a recursive value and therefore cannot be
encoded. Note that other non-recursive schemas can be encoded just fine, even
if the `Schema` datatype itself is recursive.

`Person3` contains an example of circular data which cannot be encoded with this library.

How alternatives work
----------------------

A `TypedSchema a` is really a forest of schemas, where the `TAllOf` constructor
introduces a new subtree. This allows to provide multiple schemas for a datatype,
allowing for backwards incompatible changes. The multiple schemas are tried in
order, and the first successful one is used.

A `Schema`, on the other hand, is a single tree. It follows that the functions
`encodeToWith` and `decodeFromWith` perform a search to match the source and
target schemas, where one of them is a forest. This search is potentially
exponential if one tries to find the optimal match, so instead the search is
greedy and the first match is used even if it's not optimal.

The fields `education` and `age` in the `Person2` example use alternatives to
encode non backwards compatible changes, and the commentary at the bottom of
the module shows the forest of schemas that comes out.

I now think that alternatives create more problems than they solve
and should probably be removed:

- The optimal search algorithm has exponential complexity, which forced me to use
  an `IterT` monad to treat non-termination as an effect. I could only make this
  work for decoding, introduces overhead and complexity, etc.,

- Eventually I gave up on optimal search and instead switched to a greedy
  algorithm, making the `<>` operator non-commutative and leading to human error.
  The `IterT` stayed as non-termination is still an issue for circular data and
  recursive schemas.

- Mismatch errors are crap, even for schemas that don't use alternatives,

- The denotational implementation (see next section) cannot be used.

How projections work
---------------------

Given a source schema S and a target schema T such that S is a subtype of T,
the `isSubtypeOf` function provides a coercion as evidence of the subtyping
relation.

Denotationally, `encodeToWith` is equivalent to a normal `encodeWith` (from S to S)
followed by a cast using the subtyping coercion. Similarly, `decodeFromWith`
applies the coercion, and then decodes.

This denotation used to match the implementation very closely in schemas 0.2.0,
see https://github.com/pepeiborra/schemas/blob/v0.2.0.2/src/Schemas/Internal.hs
This is not the case anymore and `encodeToWith`/`decodeFromWith` perform their
own subtyping checking, which is ver unfortunate. To see why, consider the
implications of alternatives:

- For encoding, the denotational approach would involve creating the forest of
all possible JSON values, which would be very inefficient.
- For decoding, the denotational approach would have similarly involve creating
a forest of casts.
